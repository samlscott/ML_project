---
# IMPORTANT: Change settings here, but DO NOT change the spacing.
title: "Data Science: Machine Learning"
subtitle: "Predicting Heart Disease"
documentclass: "elsarticle"
Thesis_FP: FALSE
# Entry1: "An unbelievable study with a title spanning multiple lines."
# Entry2: "\\textbf{Nico Katzke}" # textbf for bold
# Entry3: "A thesis submitted toward the degree of Doctor of Philosophy"
# Uni_Logo: Tex/Logo.png # Place a logo in the indicated location (from your root, e.g. defaults to ~/Tex/Logo.png) and uncomment this line. Leave uncommented for no image
# Logo_width: 0.3 # If using a logo - use this to set width (size) of image
# Entry4: "Under the supervision of: \\vfill Prof. Joe Smith and Dr. Frank Smith"
# Entry5: "Stellenbosch University"
# Entry6: April 2020
# Entry7:
# Entry8:
# Comment: ----- Follow this pattern for up to 5 authors
AddTitle: TRUE # Use FALSE when submitting to peer reviewed platform. This will remove author names.
Author1: "Samantha Scott"  # First Author - note the thanks message displayed as an italic footnote of first page.
Ref1: "Stellenbosch University, Cape Town, South Africa" # First Author's Affiliation
Email1: "20945043\\@sun.ac.za" # First Author's Email address
keywords: "Machine Learning \\sep Heart Disease Prediction \\sep Random Forests" # Use \\sep to separate
#JELCodes: "L250 \\sep L100"
# ----- Manage headers and footers:
#BottomLFooter: $Title$
#BottomCFooter:
#TopLHeader: \leftmark # Adds section name at topleft. Remove comment to add it.
BottomRFooter: "\\footnotesize Page \\thepage" # Add a '#' before this line to remove footer.
addtoprule: TRUE
addfootrule: TRUE               # Use if footers added. Add '#' to remove line.
# --------- page margins:
margin: 2.3 # Sides
bottom: 2 # bottom
top: 2.5 # Top
HardSet_layout: TRUE # Hard-set the spacing of words in your document. This will stop LaTeX squashing text to fit on pages, e.g.
linenumbers: FALSE # Used when submitting to journal
# ---------- References settings:
# You can download cls format here: https://www.zotero.org/ - simply search for your institution. You can also edit and save cls formats here: https://editor.citationstyles.org/about/
bibliography: Tex/ref.bib       # Do not edit: Keep this naming convention and location.
csl: Tex/harvard-stellenbosch-university.csl # referencing format used.
# By default, the bibliography only displays the cited references. If you want to change this, you can comment out one of the following:
#nocite: '@*' # Add all items in bibliography, whether cited or not
# nocite: |  # add specific references that aren't cited
# ---------- General:
RemovePreprintSubmittedTo: TRUE  # Removes the 'preprint submitted to...' at bottom of titlepage
Journal: "Journal of Finance"   # Journal that the paper will be submitting to, if RemovePreprintSubmittedTo is set to TRUE.
toc: TRUE                       # Add a table of contents
numbersections: TRUE             # Should sections (and thus figures and tables) be numbered?
fontsize: 11pt                  # Set fontsize
linestretch: 1.2                # Set distance between lines.
link-citations: TRUE            # This creates dynamic links to the papers in reference list.
### Adding additional latex packages:
# header-includes:
output:
  pdf_document:
    keep_tex: TRUE
    keep_md: TRUE
    template: Tex/TexDefault.txt
    fig_width: 3.5 # Adjust default figure sizes. This can also be done in the chunks of the text.
    fig_height: 3.5

#abstract: |
  #Abstract to be written here. The abstract should not be too long and should provide the reader with a good understanding what you are writing about. Academic papers are not like novels where you keep the reader in suspense. To be effective in getting others to read your paper, be as open and concise about your findings here as possible. Ideally, upon reading your abstract, the reader should feel he / she must read your paper in entirety.
editor_options: 
  markdown: 
    wrap: 72
---

<!-- First: Set your default preferences for chunk options: -->

<!-- If you want a chunk's code to be printed, set echo = TRUE. message = FALSE stops R printing ugly package loading details in your final paper too. I also suggest setting warning = FALSE and checking for warnings in R, else you might find ugly warnings in your paper. -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf.
# These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!

# Lets load in example data, and see how this can be stored and later called from your 'data' folder.
if(!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
Example_data <- Texevier::Ex_Dat

# Notice that as you are working in a .Rproj file (I am assuming you are) - the relative paths of your directories start at your specified root.
# This means that when working in a .Rproj file, you never need to use getwd() - it is assumed as your base root automatically.
#write_rds(Example_data, path = "data/Example_data.rds")

```


```{r}
# Loading packages
library(stats)
library(dplyr)
library(randomForest)
library(Hmisc)
library(ggcorrplot)   
library(ggplot2)  
library(caret)   
library(vip) 
library(corrplot)
library(jtools)
library(huxtable)
library(rsample)
library(visdat)  
library(recipes) 
library(ggpubr)
library(kernlab)
library(varImp)
library(ranger)
```

```{r}
# Loading data
labelschanged <- read_csv("/Users/samanthascott/Desktop/ML_project/ML_data/heart_pred/labchanged.csv")
values <- read_csv("/Users/samanthascott/Desktop/ML_project/ML_data/heart_pred/values.csv")

heart_d <- merge(labelschanged, values, by = "patient_id")

heart_d <- subset(heart_d, select = -c (patient_id))
```

```{r}
# for internet method - RFM
heart_d$heart_disease_present = as.factor(heart_d$heart_disease_present)
```

```{r}
# Split data into Training and Testing
set.seed(123) 
index_1 <- sample(1:nrow(heart_d), round(nrow(heart_d) * 0.7))
train_1 <- heart_d[index_1, ]
test_1  <- heart_d[-index_1, ]
```

# Introduction

The following paper is a comparison between two Machine Learning algorithms, namely Random Forests and Support Vector Machines, as prediction tools. Using a Linear Regression model as a baseline, the RMSE scores are compared.

# Research Question

Problem type: supervised binomial classification 

"Much like EDA, the ML process is very iterative and heurstic-based. With minimal knowledge of the problem or data at hand, it is difficult to know which ML method will perform best. This is known as the no free lunch theorem for ML (Wolpert 1996). Consequently, it is common for many ML approaches to be applied, evaluated, and modified before a final, optimal model can be determined. Performing this process correctly provides great confidence in our outcomes. If not, the results will be useless and, potentially, damaging.1"

"RMSE: Root mean squared error. This simply takes the square root of the MSE metric  so that your error is in the same units as your response variable. If your response variable units are dollars, the units of MSE are dollars-squared, but the RMSE will be in dollars. Objective: minimize"

# Data and Methodology 

The data used in this investigation is heart disease data from Kaggle. 

"Support vector machines (SVMs) offer a direct approach to binary classification: try to find a hyperplane in some feature space that “best” separates the two classes. In practice, however, it is difficult (if not impossible) to find a hyperplane to perfectly separate the classes using just the original features. SVMs overcome this by extending the idea of finding a separating hyperplane in two ways: (1) loosen what we mean by “perfectly separates”, and (2) use the so-called kernel trick to enlarge the feature space to the point that perfect separation of classes is (more) likely."

# Results and Discussion

## Random Forests

```{r}
# Generating RF Model (internet)
RFM = randomForest(heart_disease_present~., data = train_1, ntree=500, importance=TRUE)
plot(RFM)
```

```{r}
# Evaluate model accuracy
Heart_pred = predict(RFM, test_1)
test_1$Heart_pred = Heart_pred
```

```{r}
# Build confusion matrix
CFM = table(test_1$heart_disease_present, test_1$Heart_pred)
fourfoldplot(CFM, color = c("coral1", "cadet blue"),
             conf.level = 0, margin = 1, main = "Confusion Matrix")
```

```{r}
# Accuracy of the model
Classification_Accuracy = sum(diag(CFM)/sum(CFM))
Classification_Accuracy
```


```{r}
floor(sqrt(ncol(train_1) - 1))
```

```{r}
mtry <- tuneRF(train_1[-1],train_1$heart_disease_present, ntreeTry=500,
               stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
```

```{r}
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
```

```{r}
print(mtry)
print(best.m)
```

```{r}
RFM1 = randomForest(heart_disease_present~., data = train_1, mtry=best.m, ntree=500, importance=TRUE)
plot(RFM1)
```

```{r}
# Evaluate model accuracy
Heart_pred1 = predict(RFM1, test_1)
test_1$Heart_pred1 = Heart_pred1
```

```{r}
# Build confusion matrix
CFM1 = table(test_1$heart_disease_present, test_1$Heart_pred1)
fourfoldplot(CFM1, color = c("coral1", "cadet blue"),
             conf.level = 0, margin = 1, main = "Confusion Matrix")
```

```{r}
# Accuracy of the model
Classification_Accuracy1 = sum(diag(CFM1)/sum(CFM1))
Classification_Accuracy1
```

```{r}
pred1=predict(RFM,type = "prob")
library(ROCR)
perf = prediction(pred1[,2], train_1$heart_disease_present)
# 1. Area under curve
auc = performance(perf, "auc")
# 2. True Positive and Negative Rate
pred3 = performance(perf, "tpr","fpr")
# 3. Plot the ROC curve
plot(pred3,main="ROC Curve for Random Forest",col=2,lwd=2)
abline(a=0,b=1,lwd=2,lty=2,col="gray")
```

```{r}
pred2=predict(RFM1,type = "prob")
library(ROCR)
perf = prediction(pred2[,2], train_1$heart_disease_present)
# 1. Area under curve
auc = performance(perf, "auc")
# 2. True Positive and Negative Rate
pred4 = performance(perf, "tpr","fpr")
# 3. Plot the ROC curve
plot(pred4,main="ROC Curve for Random Forest",col=2,lwd=2)
abline(a=0,b=1,lwd=2,lty=2,col="gray")
```


```{r}
#caret::varImp(RFM1)
gvar <- as.data.frame(caret::varImp(RFM))
gvar1 <- as.data.frame(varImpPlot(RFM))
#rename(gvar, MeanDecreaseAccuracy = 0)
gvar1[,3] <- rownames(gvar1)
```


```{r}
ggplot(gvar1, aes(x=reorder(V3, MeanDecreaseAccuracy), y=MeanDecreaseAccuracy )) + 
  geom_point(colour = "steel blue") +
  geom_segment(aes(x=V3,xend=V3,y=0,yend=MeanDecreaseAccuracy), colour = "steel blue") +
  scale_color_discrete(name="Variable Group") +
  ylab("Mean Decrease Accuracy") +
  xlab("Variable Name") +
  coord_flip()
```

## Support Vector Machine

```{r}
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

svm_Linear <- train(heart_disease_present ~., data = train_1, method = "svmLinear",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
```

```{r}
test_pred = predict(svm_Linear, test_1)
test_1$test_pred = test_pred
```

```{r}
confusionM2 = confusionMatrix(table(test_1$test_pred, test_1$heart_disease_present))
fourfoldplot(confusionM2$table, color = c("coral1", "cadet blue"),
             conf.level = 0, margin = 1, main = "Confusion Matrix")
```

```{r}
# Accuracy of the model
Classification_AccuracyS = sum(diag(confusionM2$table)/sum(confusionM2$table))
Classification_AccuracyS
```

```{r}
grid <- expand.grid(C = c(0,0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2,5))
svm_Linear_Grid <- train(heart_disease_present ~., data = train_1, method = "svmLinear",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneGrid = grid,
tuneLength = 10)
plot(svm_Linear_Grid)
```

```{r}
test_pred_grid = predict(svm_Linear_Grid, test_1)
test_1$test_pred_grid = test_pred_grid
```

```{r}
confusionMatrix = confusionMatrix(table(test_1$test_pred_grid, test_1$heart_disease_present))
fourfoldplot(confusionMatrix$table, color = c("coral1", "cadet blue"),
             conf.level = 0, margin = 1, main = "Confusion Matrix")
```

```{r}
# Accuracy of the model
Classification_AccuracyS2 = sum(diag(confusionMatrix$table)/sum(confusionMatrix$table))
Classification_AccuracyS2
```


# Conclusion 

# Reference List 

# Appendix 

```{r}
# response distribution -> even enough, can use simple random sampling
table(heart_d$heart_disease_present) %>% prop.table()
```

```{r}
# number of features
n_features <- length(setdiff(names(train_1), "heart_disease_present"))

# train a default random forest model
rf1 <- ranger(
    heart_disease_present ~ ., 
    data = train_1,
    mtry = floor(n_features / 3),
    respect.unordered.factors = "order",
    seed = 123
)

# get OOB RMSE
(default_rmse <- sqrt(rf1$prediction.error))
```

```{r}
# number of features
n_features <- length(setdiff(names(train_1), "heart_disease_present"))

# train a default random forest model
rf2 <- ranger(
    heart_disease_present ~ ., 
    data = train_1,
    mtry = 2,
    respect.unordered.factors = "order",
    seed = 123
)

# get OOB RMSE
(default_rmse <- sqrt(rf2$prediction.error))
```